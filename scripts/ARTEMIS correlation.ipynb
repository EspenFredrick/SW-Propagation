{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "\n",
    "# Pyspedas libraries\n",
    "import pyspedas\n",
    "from pytplot import del_data, get_data, get_timespan, store_data, tplot_options, tplot_names, tplot, tplot_math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def import_events(path, satellite): # Function to import the data from a satellite to compare with Artemis: either OMNI, THEMIS, or MMS\n",
    "    probe = [] # Initialize probe ID array\n",
    "    bowshock_times = [] # Initialize start and stop time array for 'satellite'\n",
    "    lunar_times = [] # Initialize start and stop time array for ARTEMIS\n",
    "\n",
    "    with open(path, newline='') as events:\n",
    "        rows = csv.reader(events)\n",
    "        next(rows)\n",
    "        for r in rows:\n",
    "            bowshock_times.append([(dt.datetime.strptime(r[0], '%Y-%m-%dT%H:%M:%S.%fZ')+dt.timedelta(minutes=30)).strftime('%Y-%m-%d/%H:%M:%S'), (dt.datetime.strptime(r[1], '%Y-%m-%dT%H:%M:%S.%fZ')).strftime('%Y-%m-%d/%H:%M:%S')])\n",
    "            lunar_times.append([(dt.datetime.strptime(r[0], '%Y-%m-%dT%H:%M:%S.%fZ')).strftime('%Y-%m-%d/%H:%M:%S'), (dt.datetime.strptime(r[1], '%Y-%m-%dT%H:%M:%S.%fZ')).strftime('%Y-%m-%d/%H:%M:%S')])\n",
    "            if satellite == 'mms':\n",
    "                probe.append(r[2])\n",
    "            elif satellite == 'themis':\n",
    "                probe.append(str(r[3]))\n",
    "            elif satellite == 'omni':\n",
    "                probe.append('o')\n",
    "\n",
    "        if satellite == 'omni':\n",
    "            keys = np.arange(len(probe)) # Create dictionary keys 0...n equal to the number of input events\n",
    "            types = ('time', 'bx', 'by', 'bz', 'vx', 'vy', 'vz', 'n', 'T') # Keys for the sub-dictionary in each event\n",
    "            data = dict((i, dict.fromkeys(types)) for i in keys) # Create the 'data' dictionary\n",
    "            for i in keys: # Iterate through every event\n",
    "                import_data = pyspedas.omni.data(trange=bowshock_times[i], datatype='1min', level='hro2', time_clip=True) # Import the tplot variable for the ith event\n",
    "                varnames = ['BX_GSE', 'BY_GSE', 'BZ_GSE', 'Vx', 'Vy', 'Vz', 'proton_density', 'T']\n",
    "                for p in varnames:\n",
    "                    tplot_math.interp_nan(p)\n",
    "                products = [get_data('IMF')[0], get_data('BX_GSE')[1], get_data('BY_GSE')[1], get_data('BZ_GSE')[1], get_data('Vx')[1], get_data('Vy')[1], get_data('Vz')[1], get_data('proton_density')[1], get_data('T')[1]] # Each event in the 'data' dictionary will consist of these products\n",
    "                for j, k in enumerate(types): # Now go through the keys of the sub-dictionary\n",
    "                    data[i][k] = products[j] # For the first sub-dict key, set that equal to the first product (and so on)...\n",
    "\n",
    "                data[i]['time'] = data[i]['time'].astype('object')\n",
    "                for n in range(len(data[i]['time'])):\n",
    "                    data[i]['time'][n] = dt.datetime.utcfromtimestamp(data[i]['time'][n])\n",
    "\n",
    "\n",
    "        if satellite == 'themis':\n",
    "            keys = np.arange(len(probe)) # Create dictionary keys 0...n equal to the number of input events\n",
    "            fgm_types = ('time', 'bx', 'by', 'bz') # Keys for the sub-dictionary in each event\n",
    "            esa_types = ('time', 'vx', 'vy', 'vz', 'n', 'T')\n",
    "            themis_fgm_data = dict((i, dict.fromkeys(fgm_types)) for i in keys) # Create the 'fgm_data' dictionary\n",
    "            themis_esa_data = dict((i, dict.fromkeys(esa_types)) for i in keys) # Create the 'esa_data' dictionary\n",
    "            for i in keys:\n",
    "                themis_fgm_import = pyspedas.themis.fgm(trange=bowshock_times[i], probe=probe[i], time_clip=True, varnames='th'+probe[i]+'_fgs_gse')\n",
    "                themis_esa_import = pyspedas.themis.esa(trange=bowshock_times[i], probe=probe[i], time_clip=True, varnames=['th'+probe[i]+'_peif_density', 'th'+probe[i]+'_peif_avgtemp', 'th'+probe[i]+'_peif_velocity_gse'])\n",
    "                fgm_products = [get_data('th'+probe[i]+'_fgs_gse')[0], get_data('th'+probe[i]+'_fgs_gse')[1][:,0], get_data('th'+probe[i]+'_fgs_gse')[1][:,1], get_data('th'+probe[i]+'_fgs_gse')[1][:,2]]\n",
    "                esa_products = [get_data('th'+probe[i]+'_peif_velocity_gse')[0], get_data('th'+probe[i]+'_peif_velocity_gse')[1][:,0], get_data('th'+probe[i]+'_peif_velocity_gse')[1][:,1], get_data('th'+probe[i]+'_peif_velocity_gse')[1][:,2], get_data('th'+probe[i]+'_peif_density')[1], get_data('th'+probe[i]+'_peif_avgtemp')[1]]\n",
    "                for j, k in enumerate(fgm_types):\n",
    "                    themis_fgm_data[i][k] = fgm_products[j]\n",
    "                for m, n in enumerate(esa_types):\n",
    "                    themis_esa_data[i][n] = esa_products[m]\n",
    "\n",
    "        keys = np.arange(len(probe)) # Create dictionary keys 0...n equal to the number of input events\n",
    "        fgm_types = ('time', 'bx', 'by', 'bz') # Keys for the sub-dictionary in each event\n",
    "        esa_types = ('time', 'vx', 'vy', 'vz', 'n', 'T')\n",
    "        artemis_fgm_data = dict((i, dict.fromkeys(fgm_types)) for i in keys) # Create the 'fgm_data' dictionary\n",
    "        artemis_esa_data = dict((i, dict.fromkeys(esa_types)) for i in keys) # Create the 'esa_data' dictionary\n",
    "        for i in keys:\n",
    "            artemis_fgm_import = pyspedas.themis.fgm(trange=lunar_times[i], probe='b', time_clip=True, varnames='thb_fgs_gse')\n",
    "            artemis_esa_import = pyspedas.themis.esa(trange=lunar_times[i], probe='b', time_clip=True, varnames=['thb_peif_density', 'thb_peif_avgtemp', 'thb_peif_velocity_gse'])\n",
    "            fgm_products = [get_data('thb_fgs_gse')[0], get_data('thb_fgs_gse')[1][:,0], get_data('thb_fgs_gse')[1][:,1], get_data('thb_fgs_gse')[1][:,2]]\n",
    "            esa_products = [get_data('thb_peif_velocity_gse')[0], get_data('thb_peif_velocity_gse')[1][:,0], get_data('thb_peif_velocity_gse')[1][:,1], get_data('thb_peif_velocity_gse')[1][:,2], get_data('thb_peif_density')[1], get_data('thb_peif_avgtemp')[1]]\n",
    "            for j, k in enumerate(fgm_types):\n",
    "                artemis_fgm_data[i][k] = fgm_products[j]\n",
    "            for j, k in enumerate(esa_types):\n",
    "                artemis_esa_data[i][k] = esa_products[j]\n",
    "\n",
    "            artemis_fgm_data[i]['time'] = artemis_fgm_data[i]['time'].astype('object')\n",
    "            artemis_esa_data[i]['time'] = artemis_esa_data[i]['time'].astype('object')\n",
    "            for n in range(len(artemis_fgm_data[i]['time'])):\n",
    "                artemis_fgm_data[i]['time'][n] = dt.datetime.utcfromtimestamp(artemis_fgm_data[i]['time'][n])\n",
    "            for n in range(len(artemis_esa_data[i]['time'])):\n",
    "                artemis_esa_data[i]['time'][n] = dt.datetime.utcfromtimestamp(artemis_esa_data[i]['time'][n])\n",
    "\n",
    "\n",
    "\n",
    "    if satellite == 'omni':\n",
    "        return data, artemis_fgm_data, artemis_esa_data\n",
    "    if satellite == 'themis':\n",
    "        return themis_fgm_data, themis_esa_data, artemis_fgm_data, artemis_esa_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "omni, a_fgm, a_esa = import_events('../eventlist/eventlist.csv', 'omni') # Use this if you wish to compare OMNI\n",
    "#t_fgm, t_esa, a_fgm, a_esa = import_events('../eventlist/eventlist.csv', 'themis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell defines the averaging function for reducing ARTEMIS and THEMIS to a 1-minute cadence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def average(times, data):\n",
    "    minute = times[0].minute # Set the current first minute of the data set\n",
    "    timeAvgs = [] # Create empty array to store the time-averaged values\n",
    "    avgArr = [] # Create empty storage array\n",
    "    timeStep = [] # Create empty time step array\n",
    "    for i in range(len(times)): # Index the values\n",
    "        if times[i].minute == minute: # If the time of the next value equals the one set for the minute\n",
    "            avgArr.append(data[i]) # Append this to the storage array\n",
    "        elif times[i].minute == minute + 1: # If the time of the next value equals the next minute\n",
    "            #print(avgArr)\n",
    "            timeAvgs.append(np.average(avgArr)) # Average the storage array and append it to the time-averaged value array\n",
    "            #print(np.average(avgArr))\n",
    "            timeStep.append(dt.datetime(times[i-1].year, times[i-1].month, times[i-1].day, times[i-1].hour, times[i-1].minute, 00)) # Create a timestamp for the previous minute centered at 0s to line up with the orbit timestamps\n",
    "            minute= times[i].minute # Set the new current minute to start averaging over\n",
    "            avgArr = [] # Clear the storage array\n",
    "        elif times[i].minute == minute - 59: # This is for rollover: when the next minute is 0\n",
    "            #print(avgArr)\n",
    "            timeAvgs.append(np.average(avgArr))\n",
    "            #print(np.average(avgArr))\n",
    "            timeStep.append(dt.datetime(times[i-1].year, times[i-1].month, times[i-1].day, times[i-1].hour, times[i-1].minute, 00))\n",
    "            minute = times[i].minute\n",
    "            avgArr = []\n",
    "    timeAvgs.append(np.average(avgArr))\n",
    "    timeStep.append(dt.datetime(times[i].year, times[i].month, times[i].day, times[i].hour, times[i].minute, 00))\n",
    "\n",
    "    return timeStep, timeAvgs # Return the time-averaged array and the timestamp array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "fgm_to_avg = ('bx', 'by', 'bz') # List of variables to average over\n",
    "\n",
    "for i in a_fgm: # For each event\n",
    "    for j in fgm_to_avg: # For each variable to average in an event\n",
    "        time_storeage, a_fgm[i][j] = average(a_fgm[i]['time'], a_fgm[i][j]) # Store the timestamp, overwrite the variable in the library\n",
    "    a_fgm[i]['time'] = time_storeage # Overwrite the timestamps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Correlation Coefficient calculator:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def correlate(bowshock, lunar):\n",
    "\n",
    "    keys = np.arange(len(bowshock)-1) # For some reason, it's not working for all OMNI\n",
    "    output = np.column_stack(('start', 'stop', 'bx', 'by', 'bz', 'bx_t (min)', 'by_t (min)', 'bz_t (min)', 'avg_bx (nT)', 'avg_by (nT)', 'avg_bz (nT)')) # Headers for the output\n",
    "\n",
    "\n",
    "    for i in keys: # Iterate through all the events\n",
    "        for j in range(len(bowshock[i]['time'])-61): # Take the number of data points and subtract an hour from it. That's how many times you can iterate a one-hour chunk\n",
    "            lunar_start = lunar[i]['time'].index(bowshock[i]['time'][j]) # Set the start time of the sliding index where the timestamp of the fixed series begins\n",
    "            lunar_stop = lunar[i]['time'].index(bowshock[i]['time'][j+60]) # Set the end of the sliding index an hour after the start of the fixed series\n",
    "\n",
    "            store_x = [] # Coefficient storage arrays\n",
    "            store_y = []\n",
    "            store_z = []\n",
    "            for k in range(30): # Calculate a coefficient over a 30 min window\n",
    "                coef_x = np.corrcoef(bowshock[i]['bx'][j:j+60], lunar[i]['bx'][lunar_start-k:lunar_stop-k], 1)[0, 1]\n",
    "                if coef_x < 0:\n",
    "                    coef_x = 0\n",
    "                coef_y = np.corrcoef(bowshock[i]['by'][j:j+60], lunar[i]['by'][lunar_start-k:lunar_stop-k], 1)[0, 1]\n",
    "                if coef_y < 0:\n",
    "                    coef_y = 0\n",
    "                coef_z = np.corrcoef(bowshock[i]['bz'][j:j+60], lunar[i]['bz'][lunar_start-k:lunar_stop-k], 1)[0, 1]\n",
    "                if coef_z < 0:\n",
    "                    coef_z = 0\n",
    "\n",
    "                store_x.append(coef_x) # Append the storage arrays\n",
    "                store_y.append(coef_y)\n",
    "                store_z.append(coef_z)\n",
    "\n",
    "            # Create a row where each column corresponds to an arbitrary one-hour block\n",
    "            metadata = np.column_stack((bowshock[i]['time'][j], bowshock[i]['time'][j+60], max(store_x), max(store_y), max(store_z), store_x.index(max(store_x)), store_y.index(max(store_y)), store_z.index(max(store_z)), np.average(bowshock[i]['bx'][j:j+60]), np.average(bowshock[i]['by'][j:j+60]), np.average(bowshock[i]['bz'][j:j+60])))\n",
    "\n",
    "            output = np.vstack((output, metadata)) #Stack each block as you iterate\n",
    "\n",
    "    return output # Output the entire chunk of metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "a = correlate(omni, a_fgm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open('../metadata/Artemis-Omni.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f, delimiter=',')\n",
    "    csvwriter.writerows(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backup Correlation Stuff:\n",
    "---\n",
    "\n",
    "def calculate_corrs(series_fixed, series_fixed_t, series2, series2T): # Series 1 is the \"fixed\" series close to Earth, Series 2 is the one we slide\n",
    "    c=[] # Define a matrix to store correlation coefficient\n",
    "    pmax = []\n",
    "    pmax_dt = []\n",
    "    avg_bz = []\n",
    "\n",
    "    for j in range(len(series_fixed_t) - 60): # Range for the length of the fixed series minus one hour\n",
    "        a = [] # Temporary storage array\n",
    "        n2_start = series2T.index(series_fixed_t[j]) # The starting index of the sliding array is the one where the two timestamps match up\n",
    "        n2_stop = series2T.index(series_fixed_t[j+59]) # The stop index of the sliding is 59 minutes after the start\n",
    "        for i in range(30): # 30-minute sliding window\n",
    "            coef = np.corrcoef(series_fixed[j:j+59], series2[n2_start-i:n2_stop-i], 1)[0, 1] #Calculate the pearson coef\n",
    "            if coef < 0:\n",
    "                coef = 0\n",
    "            a.append(coef) # Append the storage array with the coefficient\n",
    "\n",
    "\n",
    "        pmax.append(max(a)) # Find the maximum correlation coefficient\n",
    "        pmax_dt.append(a.index(max(a))) # Find the dt at which this max coefficient occured at\n",
    "        avg_bz.append(np.average(series_fixed[j:j+59]))\n",
    "\n",
    "        c.append(a) # Append storage array c with the correlation coefficient array for that hour\n",
    "\n",
    "    return np.column_stack((pmax, pmax_dt, avg_bz))\n",
    " ---\n",
    "     vars = ['bx', 'by', 'bz']\n",
    "for v in vars:\n",
    "\n",
    "    for i in keys:\n",
    "        for j in range(len(bowshock[i]['time'])-61):\n",
    "            lunar_start = lunar[i]['time'].index(bowshock[i]['time'][j])\n",
    "            lunar_stop = lunar[i]['time'].index(bowshock[i]['time'][j+60])\n",
    "\n",
    "            datapts = []\n",
    "            store = []\n",
    "            for k in range(30):\n",
    "                coef = np.corrcoef(bowshock[i][v][j:j+60], lunar[i][v][lunar_start-k:lunar_stop-k], 1)[0, 1]\n",
    "                store.append(coef)\n",
    "\n",
    "            metadata[i][v].append(max(store))\n",
    "            metadata[i][v+'_t'].append(store.index(max(store)))\n",
    "            metadata[i]['avg_'+v].append(np.average(bowshock[i][v][j:j+60]))\n",
    "\n",
    "            print('iteration '+str(j)+' done with start = '+str(bowshock[i]['time'][j])+' stop = '+str(bowshock[i]['time'][j+60])+' for '+v)\n",
    "\n",
    "\n",
    "            #metadata[i]['start'].bowshock[i]['time'][j]\n",
    "            #metadata[i]['stop'][j] = bowshock[i]['time'][j+59]\n",
    "\n",
    "            metadata[i]['bx'].append(max(store_x))\n",
    "        metadata[i]['by'].append(max(store_y))\n",
    "        metadata[i]['bz'].append(max(store_z))\n",
    "\n",
    "        metadata[i]['bx_t'].append(store_x.index(max(store_x)))\n",
    "        metadata[i]['by_t'].append(store_y.index(max(store_y)))\n",
    "        metadata[i]['bz_t'].append(store_z.index(max(store_z)))\n",
    "\n",
    "        metadata[i]['avg_bx'].append(np.average(bowshock[i]['bx'][j:j+60]))\n",
    "        metadata[i]['avg_by'].append(np.average(bowshock[i]['by'][j:j+60]))\n",
    "        metadata[i]['avg_bz'].append(np.average(bowshock[i]['bz'][j:j+60]))\n",
    "\n",
    "        metadata[i]['start'].append(bowshock[i]['time'][j])\n",
    "        metadata[i]['stop'].append(bowshock[i]['time'][j+60])\n",
    "\n",
    "        parameters = ['start', 'stop', 'bx', 'by', 'bz', 'bx_t', 'by_t', 'bz_t', 'avg_bx', 'avg_by', 'avg_bz']\n",
    "metadata = dict((i, dict((j, []) for j in parameters)) for i in keys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
